pipeline:
  # The 'pipeline' section defines some high-level parameters that are mostly just used
  # to name the files that this pipeline produces. Our file naming conventions dictate
  # that output (processed) files are named like:
  # <location_id>.<dataset_name>[-qualifier][-temporal].<data_level>.<YYYYMMDD>.<HHMMSS>.<extension>

  type: Ingest
  location_id: "pnnl"
  dataset_name: "example_ingest"
  # qualifier: ""
  # temporal: ""
  data_level: "b1" # If not applying QC this should be set to "a1"

# --------------------------------------------------------------------------------------

dataset_definition:
  # The 'dataset_definition' section describes the metadata and variable structure that
  # the output dataset will take. This section is based on the netCDF standard such that
  # for each output dataset there are global attributes, dimensions, coordinate
  # variables, and data variables. The actual structure the output dataset takes is
  # determined by the output FileHandler selected in the storage configuration file (by
  # default it is netCDF, so the structure and metadata here is preserved).

  attributes:
    # The 'attributes' section defines the global attributes that will appear in the
    # output dataset. It is recommended to include at least the attributes listed below
    # in your configuration files, and we always encourage adding more information that
    # helps your users understand the context in which the data were obtained.
    title: "Example Ingest"
    description: "This is an example ingest meant to demonstrate how one might set up an ingestion pipeline using this template repository. It should be deleted before this repository or any of its ingests are used in a production environment."
    conventions: MHKiT-Cloud Data Standards v. 1.0
    institution: Pacific Northwest National Laboratory
    code_url: https://github.com/tsdat/ingest-template
    location_meaning: "Pacific Northwest National Laboratory"

  dimensions:
    # The 'dimensions' section defines the shape of your dataset. It is expected that
    # 'time' is always a dimension, but you may have as many additional dimensions as
    # you would like.
    time:
      length: unlimited

  variables:
    # The 'variables' section defines the metadata and retrieval rules for measurements
    # that are reported in your output dataset. Each variable configuration block
    # consists of 3-5 main components:
    #
    # 1) input [optional] - tells tsdat how the variable should be retrieved. You can
    # leave this out if you want to populate the variable's data manually.
    # 2) data [optional] - used to set the variable's data directly in this config file,
    # which is useful for scalar properties (like altitude or lat/lon).
    # 3) dims – tells tsdat what shape this data has.
    # 4) type – this is the np.dtype of the variable.
    # 5) attrs – metadata that describes this variable.
    #
    # You can leave off both items 1 and 2 to have tsdat initialize the variable as an
    # empty array. This is useful if you want to add data to your output file that isn't
    # directly retrieved from the input dataset.

    time:
      # The time coordinate variable is required in every dataset processed by tsdat
      input:
        # The input section describes how the data should be retrieved from the input
        # file. Here we say that 'time' is named 'Timestamp (end of interval)' in the
        # input file. We also specify how the data should be converted from the input
        # to output; in this case we specify how to convert the time strings from the
        # input into a python datetime object.
        name: Timestamp (end of interval)
        converter:
          classname: tsdat.utils.converters.StringTimeConverter
          parameters:
            timezone: UTC
            time_format: "%Y-%m-%d %H:%M:%S"

      dims: [time] # 'time' is a coordinate variable, so it is dimensioned by 'time'

      # 'time' should be a 'long' data type, since in this case it is the number of
      # seconds since 1970, which is very common and standard way to represent time.
      type: long

      attrs:
        # Here we define some helpful attributes for users and machines. Typically you
        # should always define a long_name (used for plot labels), a standard_name (used
        # to standardize variable across datasets according to CF Conventions), and
        # units (https://github.com/hgrecco/pint/blob/master/pint/default_en.txt)
        long_name: Time (UTC)
        standard_name: time
        units: seconds since 1970-01-01T00:00:00

    example_var:
      input:
        name: "Example"
        units: km
      dims: [time]
      type: float
      attrs:
        long_name: Example Var
        units: km

    latitude:
      data: 39.90891 # You can specify the data value directly like this
      type: float
      attrs:
        long_name: "North latitude"
        standard_name: "latitude"
        comment: "Recorded lattitude at the instrument location"
        units: "degree_N"
        valid_range: [-90.f, 90.f]

    longitude:
      data: -105.22715
      type: float
      attrs:
        long_name: "East longitude"
        standard_name: "longitude"
        comment: "Recorded longitude at the instrument location"
        units: "degree_E"
        valid_range: [-180.f, 180.f]

    altitude:
      data: 1828.8
      type: float
      attrs:
        long_name: "Altitude"
        standard_name: "altitude"
        comment: "Recorded altitude at the instrument location"
        units: m

# --------------------------------------------------------------------------------------

quality_management:
  # This section of the pipeline configuration file is all about detecting and managing
  # data quality issues. Each config block has three main sections:
  #
  # 1) checker - tells tsdat what QualityChecker to use to detect quality issues
  # 2) handlers - tells tsdat which QualityHandler(s) to use to handle quality issues
  # 3) variables / exclude - tells tsdat which variables should be checked

  manage_missing_coordinates:
    checker:
      # Tells tsdat which QualityChecker should be used to identify quality issues. Here
      # we use the built-in CheckMissing class in the tsdat.qc.checkers module.
      classname: tsdat.qc.checkers.CheckMissing
    handlers:
      # Tells tsdat which QualityHandler(s) should be used to handle quality issues. In
      # this case we only use one handler, which fails the pipeline.
      - classname: tsdat.qc.handlers.FailPipeline
    variables:
      # COORDS is a keyword which tells tsdat to only run this quality manager for
      # coordinate variables, which are just variables that also are dimensions. In this
      # dataset 'time' is the only coordinate variable.
      - COORDS

  manage_coordinate_monotonicity:
    checker:
      classname: tsdat.qc.checkers.CheckMonotonic
    handlers:
      - classname: tsdat.qc.handlers.SortDatasetByCoordinate
        parameters:
          ascending: True
          correction: "Coordinate data was sorted in order to ensure monotonicity."
    variables:
      - COORDS

  manage_missing_values:
    checker:
      classname: tsdat.qc.checkers.CheckMissing

    handlers:
      # The RemoveFailedValues handler replaces values that are missing with the
      # variable's _FillValue (from CF Conventions), which defaults to -9999. Xarray and
      # other netCDF libraries will interpret this as a NaN.
      - classname: tsdat.qc.handlers.RemoveFailedValues

      # For each variable the RecordQualityResults handler is applied to, a qc variable
      # is created (or updated, if it exists) whose purpose is to track the specific
      # test(s) and data indexes that failed. It uses a bit-packing approach to store
      # the results of tests.
      - classname: tsdat.qc.handlers.RecordQualityResults
        parameters:
          bit: 1
          assessment: Bad
          meaning: "Value is equal to _FillValue or NaN"

    variables:
      # DATA_VARS is a keyword which tells tsdat to only run this quality manager for
      # data variables, which are all non-coordinate variables are dimensions. In this
      # dataset 'example_var', 'latitude', 'longitude', and 'altitude' are all data
      # variables
      - DATA_VARS

    exclude:
      # The exclude keyword tells tsdat to skip this check for specific variables that
      # would otherwise have been selected. Note that here we opt to skip the latitude,
      # longitude, and altitude variables since we set their data values in this config
      # file - no need to analyze the quality of that data, we already know it's valid.
      - latitude
      - longitude
      - altitude
